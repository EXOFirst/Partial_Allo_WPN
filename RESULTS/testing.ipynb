{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from itertools import cycle\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "\n",
    "# ------------------------------------environment----------------------------\n",
    "class UAV():\n",
    "    '''\n",
    "    <Constants AND Variables>\n",
    "        H: UAV flying height (m)\n",
    "        N: number of users\n",
    "        C: service region radius (m)\n",
    "        T: operation period of the UAV (time slots)\n",
    "        ts: time slot duration (seconds)\n",
    "        Eb: UAV energy budget per slot (J)\n",
    "        Pf: flying power of the UAV (watts)\n",
    "        Ph: hovering power of the UAV (watts)\n",
    "        v: UAV velocity (m/s)\n",
    "\n",
    "        iota: chip energy consumption coefficient (kappa also same)\n",
    "        FU: number of UAV CPU cycle per bit (bits)\n",
    "        FI: number of GUs CPU cycle per bit (bits)\n",
    "        Fil: computing capacity of GUs\n",
    "        Fus: computing capacuty of UAV -> 1Ghz\n",
    "        D: input data size (bits)\n",
    "\n",
    "        B: system bandwidth (MHz)\n",
    "        beta: channel power gain at reference distance d0 = 1 m (dB)\n",
    "        alpha: path-loss exponent\n",
    "        sigma: noise power spectral density (dBm/Hz)\n",
    "\n",
    "        Emax: battery capacity of the user, (mJ)\n",
    "        mu: average harvested energy (mJ)\n",
    "\n",
    "        eta = energy conversion efficiency \n",
    "        ppb = transmit power at the Power Beacon (watts)\n",
    "\n",
    "        wis: ground user(GU) locate\n",
    "        tau: harvest time\n",
    "        pis: GU transmit power (mW)\n",
    "\n",
    "        w: weight (it will change(just simulate it))\n",
    "        flis: computing capacity of GUs ()\n",
    "        O: range of offlaoding ratio \n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, H=20, N=10, T=40, ts=6, Eb=8, v=15, Pf=2, Ph=1, iota=1e-27, FI=[1, 5], FU=[20, 30], D=[0.4, 1], B=30,\n",
    "                 beta=30, alpha=2, sigma=-174, Emax=10, eta=1e-28, ppb=4, pis=[1.0, 10.0], w=0.1, O=[0, 1]):\n",
    "        self.H = H\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "        self.ts = ts\n",
    "        self.Eb = Eb\n",
    "        self.Pf = Pf\n",
    "        self.Ph = Ph\n",
    "        self.v = v\n",
    "\n",
    "        self.iota = iota\n",
    "        self.FI = np.array(FI)*1e5\n",
    "        self.FU = np.array(FU)*1e5 \n",
    "        self.D = np.array(D) * (10**6)\n",
    "\n",
    "        self.B = B*1e6\n",
    "       #  self.pn = np.array(pn) *1e-3\n",
    "        self.beta = beta**(-30/10)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = 10**(sigma/10)*1e-3\n",
    "\n",
    "        self.Es = [0.1, Emax]\n",
    "\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.ppb = ppb\n",
    "        self.tau = self.T*0.9\n",
    "        self.pis = np.random.uniform(pis[0], pis[1], self.N) * 1e-3\n",
    "\n",
    "        self.wis = np.random.rand(self.N, 2)\n",
    "        self.wo = np.array([0, 0])\n",
    "        self.wb = np.array([9, 9])\n",
    "\n",
    "        self.w = w\n",
    "        self.O = np.array(O)\n",
    "\n",
    "        self.Eh = self.rf_eh()\n",
    "\n",
    "    '''\n",
    "    wo: UAV locate\n",
    "    wb: power beacon locate\n",
    "    ois_1: offloading ratio on time slot s-1 for state (uniform dist.)\n",
    "    dis: inputdata of user on time slot s (uniform dist.)\n",
    "    eus: UAV energy consumption on time slot s, will change (uniform dist.)\n",
    "    '''\n",
    "\n",
    "    def reset(self):\n",
    "        ois_1 = np.random.uniform(self.O[0], self.O[1], self.N)\n",
    "        # print(f'in reset ois_1: {ois_1} \\n')\n",
    "        Dis = np.random.uniform(\n",
    "            self.D[0], self.D[1], self.N)  # change the size\n",
    "        # print(f'in reset Dis{Dis} \\n ')\n",
    "        Eus = np.random.rand()\n",
    "        state = [ois_1, Dis, Eus]  # STATE\n",
    "\n",
    "        return state\n",
    "\n",
    "    '''\n",
    "    dis: distance between UAV and GU \n",
    "    gis: channel gain between UAV and GU\n",
    "    ris: data rate between user and UAV\n",
    "    Fi: CPU cycle about input data\n",
    "    fli: local computation capa. \n",
    "    Elc: energy consumption of local(GU)\n",
    "    tlc: execution time from GU in time slot \n",
    "    Et: energy consumption for transmission\n",
    "    ttis: local execution time at GU in time slot\n",
    "    '''\n",
    "\n",
    "    def local(self, fli, ois_1, Dis):\n",
    "        Fi = np.random.uniform(self.FI[0], self.FI[1], self.N)\n",
    "        dis = np.sqrt(self.H**2 + np.sum((self.wo-self.wis)**2))\n",
    "        gis = self.beta / (dis**self.alpha)\n",
    "        ris = self.B * np.log2(1 + ((self.pis*gis)/self.sigma**2))\n",
    "        \n",
    "        # print(f'in local \\n fli shape:{np.shape(fli)} \\n ois_1 shape:{np.shape(ois_1)} \\n Dis shape:{np.shape(Dis)} \\n')\n",
    "\n",
    "        Elc = self.iota * (fli**2) * Fi * (1-ois_1)*Dis\n",
    "        tlc = ((1-ois_1)*Dis) / fli\n",
    "\n",
    "        Et = (self.pis * (1-ois_1)*Dis) / ris\n",
    "        ttis = ((1-ois_1)*Dis)/ris\n",
    "        \n",
    "        Elis = Elc + Et\n",
    "        tlis = tlc + ttis \n",
    "        return Elis, tlis\n",
    "\n",
    "    '''\n",
    "    Eexe: energy consumption at the UAV for processing the data \n",
    "    texe: execution time at the UAV for processing the offloaded data\n",
    "    '''\n",
    "\n",
    "    def computing(self, fus, ois_1, Dis):\n",
    "        FU = np.random.randint(self.FU[0], self.FU[1],dtype='int64')\n",
    "\n",
    "        # print(f'in computing \\n fus shape: {np.shape(fus)} \\n ois_1 shape: {np.shape(ois_1)} \\n Dis shape: {np.shape(Dis)}, \\n FU: {np.shape(self.FU)} \\n')\n",
    "        # print(f'FU rand int {self.FU[0], self.FU[1]} \\n')\n",
    "        \n",
    "        Eexe = self.iota*((fus)**2)*FU*(ois_1*Dis)\n",
    "        texe = (ois_1 * Dis) / fus\n",
    "        return Eexe, texe\n",
    "        # Eexe = self.iota*((fus)^2)*self.FU\n",
    "        # texe = (ois_1 * Dis) / fus\n",
    "        # return Eexe, texe\n",
    "\n",
    "    '''\n",
    "    db2u: distace between the UAV and power beacon\n",
    "    gpb: channel gain between the UAV and power beacon\n",
    "    Eh: harvested energy  \n",
    "    '''\n",
    "\n",
    "    def rf_eh(self):\n",
    "        db2u = np.sqrt(self.H**2 + np.sum((self.wo - self.wb)**2))\n",
    "        gpb = self.beta / (db2u**self.alpha)\n",
    "        Eh = self.eta * self.ppb * gpb * self.tau\n",
    "        return Eh\n",
    "\n",
    "    '''\n",
    "    State = [ois_1, dis, eus]\n",
    "    Action = [pis, fus]\n",
    "    '''\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # will check this env.\n",
    "        # extract information from state\n",
    "        ois_1 = state[0]  # first state is randomly\n",
    "        Dis = state[1] * (self.D[1]-self.D[0]) + self.D[0]\n",
    "        Eus = state[2]\n",
    "\n",
    "        # extract information from state\n",
    "        # pis = action[0] * (self.pis[1]-self.pis[0]) + self.pis[0]\n",
    "        fus = action[0] * (self.FU[1]-self.FU[0]) + self.FU[0]\n",
    "        fli = action[1] * (self.FI[1]-self.FI[0]) + self.FI[0]\n",
    "        ois = action[2]\n",
    "\n",
    "        # need adjust Fi\n",
    "        # Fi = self.FU\n",
    "        # fli = self.\n",
    "\n",
    "        Elis, tlis = self.local(fli, ois_1, Dis)\n",
    "        Eexe, texe = self.computing(fus, ois_1, Dis)\n",
    "                \n",
    "        # --------------- reward ------------------\n",
    "        alarm = (tlis < self.ts) | (texe < self.ts)\n",
    "        # print(f'in step alarm {alarm}')\n",
    "        tlis, texe = tlis[alarm], texe[alarm]\n",
    "\n",
    "        # print(f'in step raward sum(Elis+Eexe) {np.sum(Elis + Eexe)} \\n w*tlis {self.w * abs(tlis-self.ts)} \\n w*texe {self.w * abs(texe-self.ts)} \\n')\n",
    "        # print(f'in step shapes about \\n Elis {np.shape(Elis)} \\n Eexe {np.shape(Eexe)} \\n  tlis {np.shape(tlis)}  \\ texe {np.shape(texe)}')\n",
    "        \n",
    "        reward = np.sum(Elis + (Eexe)) + self.w * np.sum(abs(tlis-self.ts)) + self.w * np.sum(abs(texe-self.ts))\n",
    "\n",
    "    \n",
    "        # print(f'in step reward {reward} \\n')\n",
    "        # ---------------  update state ---------------\n",
    "        ois_nxt = ois\n",
    "        Dis_nxt = np.random.uniform(self.D[0], self.D[1], self.N)\n",
    "        # print(f'in step Dis_nxt: {Dis_nxt} \\n')\n",
    "        Eus_nxt = Eus - np.sum(Eexe) + self.Eh\n",
    "        Eus_nxt = np.clip(Eus_nxt, self.Es[0], self.Es[1])\n",
    "        Eus_nxt = (Eus_nxt - self.Es[0]) / (self.Es[1]-self.Es[0])\n",
    "        # Eh\n",
    "\n",
    "        next_state = [ois_nxt, Dis_nxt, Eus_nxt]\n",
    "        # update state\n",
    "        return reward, next_state\n",
    "\n",
    "# ------------------------------------agent----------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_agent(env, state):\n",
    "    ois_1 = state[0]\n",
    "    Dis = state[1]\n",
    "    Eus = state[2]\n",
    "\n",
    "    alarm = True\n",
    "    while alarm:\n",
    "        FUmax = env.FU[1] * 0.9\n",
    "        FUmin = env.FU[0] * 0.9\n",
    "        FImax = env.FI[1] * 0.9\n",
    "        FImin = env.FI[0] * 0.9\n",
    "\n",
    "        fus = np.random.rand(env.N) * (FUmax-FUmin) + FUmin\n",
    "        fli = np.random.rand(env.N) * (FImax-FImin) + FImin\n",
    "        ois = np.random.rand(env.N)\n",
    "\n",
    "        tlis = env.local(fli, ois_1, Dis)[1]\n",
    "        Eexe, texe = env.computing(fus, ois_1, Dis)[0], env.computing(fus, ois_1, Dis)[1]\n",
    "\n",
    "        alarm = bool((tlis < env.ts) | (texe < env.ts) & (Eexe < Eus))\n",
    "\n",
    "    action = [fus, fli, ois]\n",
    "    return action\n",
    "\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (self.x_prev\n",
    "             + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "             + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape))\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(self, env, buffer_capacity=1000, learning_rate=0.01, batch_size=32, discount=0.9):\n",
    "        self.env = env\n",
    "        self.state_dim = 2 * env.N + 1  # why add 2? -> because it is vector\n",
    "        self.action_dim = 2 * env.N + 1\n",
    "        self.actor_learning_rate = learning_rate\n",
    "        self.critic_learning_rate = 2*learning_rate\n",
    "        self.gamma = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.train_start = 10\n",
    "        self.xi = 0.001  # for update target networks\n",
    "        self.kn_init = tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.1, maxval=0.1)\n",
    "\n",
    "        # ---- Replay memory --------------------------------------------------\n",
    "        self.buffer = deque(maxlen=self.buffer_capacity)\n",
    "\n",
    "        # ---- Creat a noise process ------------------------------------------\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.01\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.noise = OUActionNoise(mean=self.mean*np.ones(self.action_dim),\n",
    "                                   std_deviation=self.std*np.ones(self.action_dim))\n",
    "\n",
    "        # ---- Create actor and critic ----------------------------------------\n",
    "        self.actor = self.get_actor()\n",
    "        self.target_actor = self.get_actor()\n",
    "        self.critic = self.get_critic()\n",
    "        self.target_critic = self.get_critic()\n",
    "\n",
    "        # Make the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            self.critic_learning_rate)\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        # Saves experience tuple (s,a,r,s') in the replay memory\n",
    "        self.buffer.append(obs_tuple)\n",
    "\n",
    "    def get_actor(self):\n",
    "        inputs = layers.Input(shape=(self.state_dim,))\n",
    "        hidden = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "        hidden = layers.Dense(128, activation=\"relu\")(hidden)\n",
    "\n",
    "        # action = [ut, fnt, pnt, bnt]\n",
    "        action = layers.Dense(\n",
    "            self.action_dim, activation=\"sigmoid\", kernel_initializer=self.kn_init)(hidden)\n",
    "\n",
    "        # Outputs actions\n",
    "        model = keras.Model(inputs=inputs, outputs=action)\n",
    "        return model\n",
    "\n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.state_dim,))\n",
    "        state_out = layers.Dense(128, activation=\"relu\")(state_input)\n",
    "        state_out = layers.Dense(128, activation=\"relu\")(state_out)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.action_dim,))\n",
    "        action_out = layers.Dense(128, activation=\"relu\")(action_input)\n",
    "        action_out = layers.Dense(128, activation=\"relu\")(action_out)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        hidden = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        hidden = layers.Dense(256, activation=\"relu\")(hidden)\n",
    "        Qvalue = layers.Dense(1)(hidden)\n",
    "\n",
    "        # Outputs Q-value for given state-action\n",
    "        model = keras.Model(inputs=[state_input, action_input], outputs=Qvalue)\n",
    "        return model\n",
    "\n",
    "    def convert_to_vector(self, var_in):\n",
    "        # Convert a state into a vector for Tensorflow process\n",
    "        out = np.empty(0)\n",
    "        for var in var_in:\n",
    "            out = np.append(out, np.reshape(var, (1, -1)))\n",
    "        return out\n",
    "\n",
    "    def policy(self, state, scheme='Proposed'):\n",
    "        # Return an action sampled from the actor DNN plus some noise for exploration\n",
    "        # Convert the state into a vector, then to a tensor\n",
    "        state_vector = self.convert_to_vector(state)\n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state_vector), 0)\n",
    "\n",
    "        # Sample action from the actor, and add noise to the sampled actions\n",
    "        sampled_action = tf.squeeze(self.actor(tf_state))\n",
    "        sampled_action = sampled_action.numpy() + self.epsilon * self.noise()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "         # Make sure actions are within bounds\n",
    "        action = np.clip(sampled_action, 0, 1)\n",
    "\n",
    "        # actions = [ut, fnt, pnt, bnt]\n",
    "\n",
    "        # will change below\n",
    "        fus = action[0:1]\n",
    "        fli = action[1:self.env.N+1]\n",
    "        ois = action[self.env.N+1:]\n",
    "        # print(f'in policy action shape: {np.shape(action)} \\n')\n",
    "        # print(f'in  policy action {action}\\n')\n",
    "        # print(f'in policy fus: {fus} \\n fli: {fli} \\n ois: {ois} \\n')\n",
    "        # print(f'in policy \\n fus shape: {np.shape(fus)} \\n fli shape: {np.shape(fli)} \\n ois shape: {np.shape(ois)} \\n')\n",
    "\n",
    "        if scheme == 'Uniform':\n",
    "            fus = 0.9*np.random.rand(1)\n",
    "            fli = 0.9*np.random.rand(self.env.N)\n",
    "            ois = np.random.rand(0, 1, self.env.N)\n",
    "\n",
    "        if scheme == 'Random':\n",
    "            fus = 0.9*np.random.rand(1)\n",
    "            fli = np.random.rand(1, 5, self.env.N)\n",
    "            ois = np.random.rand(0, 1, self.env.N)\n",
    "\n",
    "        return [fus, fli, ois]\n",
    "\n",
    "     # Use tf.function to speed up blocks of code that contain many small TensorFlow operations.\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, state_next_batch):\n",
    "        # Train the actor and the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(state_next_batch, training=True)\n",
    "            y = reward_batch + self.gamma * \\\n",
    "                self.target_critic(\n",
    "                    [state_next_batch, target_actions], training=True)\n",
    "            critic_value = self.critic(\n",
    "                [state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "        critic_grad = tape.gradient(\n",
    "            critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch, training=True)\n",
    "            critic_value = self.critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target_actor.variables, self.actor.variables):\n",
    "            a.assign(b * self.xi + a * (1 - self.xi))\n",
    "\n",
    "        for (c, d) in zip(self.target_critic.variables, self.critic.variables):\n",
    "            c.assign(d * self.xi + c * (1 - self.xi))\n",
    "\n",
    "    def update_model(self):\n",
    "        # Select random samples from the buffer to train the actor and the critic\n",
    "        if len(self.buffer) < self.train_start:\n",
    "            return\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, state_next_batch = [], [], [], []\n",
    "        for i in indices:\n",
    "            state_batch.append(self.convert_to_vector(self.buffer[i][0]))\n",
    "            action_batch.append(self.convert_to_vector(self.buffer[i][1]))\n",
    "            reward_batch.append(self.buffer[i][2])\n",
    "            state_next_batch.append(self.convert_to_vector(self.buffer[i][3]))\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(action_batch)\n",
    "        reward_batch = tf.convert_to_tensor(reward_batch)\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        state_next_batch = tf.convert_to_tensor(state_next_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        self.update(state_batch, action_batch, reward_batch, state_next_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Learning rate ====== : 0.001\n",
      "------ Iteration: 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:65: RuntimeWarning: invalid value encountered in true_divide\n",
      "__main__:69: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ep. 0  *  Avg Reward => nan\n",
      " Ep. 1  *  Avg Reward => 0.500\n",
      " Ep. 2  *  Avg Reward => 0.541\n",
      " Ep. 3  *  Avg Reward => 0.512\n",
      " Ep. 4  *  Avg Reward => 0.492\n",
      " Ep. 5  *  Avg Reward => 0.451\n",
      " Ep. 6  *  Avg Reward => 0.414\n",
      " Ep. 7  *  Avg Reward => 0.404\n",
      " Ep. 8  *  Avg Reward => 0.429\n",
      " Ep. 9  *  Avg Reward => 0.505\n",
      " Ep. 10  *  Avg Reward => 0.518\n",
      " Ep. 11  *  Avg Reward => 0.478\n",
      " Ep. 12  *  Avg Reward => 0.473\n",
      " Ep. 13  *  Avg Reward => 0.492\n",
      " Ep. 14  *  Avg Reward => 0.506\n",
      " Ep. 15  *  Avg Reward => 0.506\n",
      " Ep. 16  *  Avg Reward => 0.499\n",
      " Ep. 17  *  Avg Reward => 0.486\n",
      " Ep. 18  *  Avg Reward => 0.491\n",
      " Ep. 19  *  Avg Reward => 0.490\n",
      " Ep. 20  *  Avg Reward => 0.491\n",
      " Ep. 21  *  Avg Reward => 0.483\n",
      " Ep. 22  *  Avg Reward => 0.489\n",
      " Ep. 23  *  Avg Reward => 0.508\n",
      " Ep. 24  *  Avg Reward => 0.465\n",
      " Ep. 25  *  Avg Reward => 0.463\n",
      " Ep. 26  *  Avg Reward => 0.474\n",
      " Ep. 27  *  Avg Reward => 0.485\n",
      " Ep. 28  *  Avg Reward => 0.488\n",
      " Ep. 29  *  Avg Reward => 0.490\n",
      " Ep. 30  *  Avg Reward => 0.497\n",
      " Ep. 31  *  Avg Reward => 0.500\n",
      " Ep. 32  *  Avg Reward => 0.515\n",
      " Ep. 33  *  Avg Reward => 0.527\n",
      " Ep. 34  *  Avg Reward => 0.533\n",
      " Ep. 35  *  Avg Reward => 0.523\n",
      " Ep. 36  *  Avg Reward => 0.531\n",
      " Ep. 37  *  Avg Reward => 0.538\n",
      " Ep. 38  *  Avg Reward => 0.548\n",
      " Ep. 39  *  Avg Reward => 0.559\n",
      " Ep. 40  *  Avg Reward => 0.564\n",
      " Ep. 41  *  Avg Reward => 0.564\n",
      " Ep. 42  *  Avg Reward => 0.562\n",
      " Ep. 43  *  Avg Reward => 0.563\n",
      " Ep. 44  *  Avg Reward => 0.563\n",
      " Ep. 45  *  Avg Reward => 0.557\n",
      " Ep. 46  *  Avg Reward => 0.551\n",
      " Ep. 47  *  Avg Reward => 0.545\n",
      " Ep. 48  *  Avg Reward => 0.540\n",
      " Ep. 49  *  Avg Reward => 0.535\n",
      " Ep. 50  *  Avg Reward => 0.534\n",
      " Ep. 51  *  Avg Reward => 0.536\n",
      " Ep. 52  *  Avg Reward => 0.536\n",
      " Ep. 53  *  Avg Reward => 0.539\n",
      " Ep. 54  *  Avg Reward => 0.540\n",
      " Ep. 55  *  Avg Reward => 0.540\n",
      " Ep. 56  *  Avg Reward => 0.536\n",
      " Ep. 57  *  Avg Reward => 0.527\n",
      " Ep. 58  *  Avg Reward => 0.523\n",
      " Ep. 59  *  Avg Reward => 0.521\n",
      " Ep. 60  *  Avg Reward => 0.513\n",
      " Ep. 61  *  Avg Reward => 0.508\n",
      " Ep. 62  *  Avg Reward => 0.509\n",
      " Ep. 63  *  Avg Reward => 0.509\n",
      " Ep. 64  *  Avg Reward => 0.506\n",
      " Ep. 65  *  Avg Reward => 0.503\n",
      " Ep. 66  *  Avg Reward => 0.500\n",
      " Ep. 67  *  Avg Reward => 0.494\n",
      " Ep. 68  *  Avg Reward => 0.520\n",
      " Ep. 69  *  Avg Reward => 0.514\n",
      " Ep. 70  *  Avg Reward => 0.508\n",
      " Ep. 71  *  Avg Reward => 0.503\n",
      " Ep. 72  *  Avg Reward => 0.502\n",
      " Ep. 73  *  Avg Reward => 0.502\n",
      " Ep. 74  *  Avg Reward => 0.505\n",
      " Ep. 75  *  Avg Reward => 0.506\n",
      " Ep. 76  *  Avg Reward => 0.504\n",
      " Ep. 77  *  Avg Reward => 0.505\n",
      " Ep. 78  *  Avg Reward => 0.504\n",
      " Ep. 79  *  Avg Reward => 0.504\n",
      " Ep. 80  *  Avg Reward => 0.505\n",
      " Ep. 81  *  Avg Reward => 0.503\n",
      " Ep. 82  *  Avg Reward => 0.501\n",
      " Ep. 83  *  Avg Reward => 0.502\n",
      " Ep. 84  *  Avg Reward => 0.501\n",
      " Ep. 85  *  Avg Reward => 0.497\n",
      " Ep. 86  *  Avg Reward => 0.493\n",
      " Ep. 87  *  Avg Reward => 0.492\n",
      " Ep. 88  *  Avg Reward => 0.495\n",
      " Ep. 89  *  Avg Reward => 0.497\n",
      " Ep. 90  *  Avg Reward => 0.498\n",
      " Ep. 91  *  Avg Reward => 0.498\n",
      " Ep. 92  *  Avg Reward => 0.496\n",
      " Ep. 93  *  Avg Reward => 0.497\n",
      " Ep. 94  *  Avg Reward => 0.496\n",
      " Ep. 95  *  Avg Reward => 0.498\n",
      " Ep. 96  *  Avg Reward => 0.499\n",
      " Ep. 97  *  Avg Reward => 0.500\n",
      " Ep. 98  *  Avg Reward => 0.499\n",
      " Ep. 99  *  Avg Reward => 0.501\n",
      " Ep. 100  *  Avg Reward => 0.503\n",
      " Ep. 101  *  Avg Reward => 0.502\n",
      " Ep. 102  *  Avg Reward => 0.498\n",
      " Ep. 103  *  Avg Reward => 0.499\n",
      " Ep. 104  *  Avg Reward => 0.501\n",
      " Ep. 105  *  Avg Reward => 0.501\n",
      " Ep. 106  *  Avg Reward => 0.500\n",
      " Ep. 107  *  Avg Reward => 0.502\n",
      " Ep. 108  *  Avg Reward => 0.502\n",
      " Ep. 109  *  Avg Reward => 0.501\n",
      " Ep. 110  *  Avg Reward => 0.501\n",
      " Ep. 111  *  Avg Reward => 0.500\n",
      " Ep. 112  *  Avg Reward => 0.499\n",
      " Ep. 113  *  Avg Reward => 0.497\n",
      " Ep. 114  *  Avg Reward => 0.496\n",
      " Ep. 115  *  Avg Reward => 0.493\n",
      " Ep. 116  *  Avg Reward => 0.492\n",
      " Ep. 117  *  Avg Reward => 0.490\n",
      " Ep. 118  *  Avg Reward => 0.489\n",
      " Ep. 119  *  Avg Reward => 0.489\n",
      " Ep. 120  *  Avg Reward => 0.490\n",
      " Ep. 121  *  Avg Reward => 0.489\n",
      " Ep. 122  *  Avg Reward => 0.489\n",
      " Ep. 123  *  Avg Reward => 0.488\n",
      " Ep. 124  *  Avg Reward => 0.488\n",
      " Ep. 125  *  Avg Reward => 0.488\n",
      " Ep. 126  *  Avg Reward => 0.486\n",
      " Ep. 127  *  Avg Reward => 0.482\n",
      " Ep. 128  *  Avg Reward => 0.482\n",
      " Ep. 129  *  Avg Reward => 0.480\n",
      " Ep. 130  *  Avg Reward => 0.480\n",
      " Ep. 131  *  Avg Reward => 0.476\n",
      " Ep. 132  *  Avg Reward => 0.474\n",
      " Ep. 133  *  Avg Reward => 0.471\n",
      " Ep. 134  *  Avg Reward => 0.471\n",
      " Ep. 135  *  Avg Reward => 0.473\n",
      " Ep. 136  *  Avg Reward => 0.474\n",
      " Ep. 137  *  Avg Reward => 0.473\n",
      " Ep. 138  *  Avg Reward => 0.472\n",
      " Ep. 139  *  Avg Reward => 0.473\n",
      " Ep. 140  *  Avg Reward => 0.474\n",
      " Ep. 141  *  Avg Reward => 0.473\n",
      " Ep. 142  *  Avg Reward => 0.473\n",
      " Ep. 143  *  Avg Reward => 0.472\n",
      " Ep. 144  *  Avg Reward => 0.470\n",
      " Ep. 145  *  Avg Reward => 0.471\n",
      " Ep. 146  *  Avg Reward => 0.470\n",
      " Ep. 147  *  Avg Reward => 0.470\n",
      " Ep. 148  *  Avg Reward => 0.470\n",
      " Ep. 149  *  Avg Reward => 0.471\n",
      " Ep. 150  *  Avg Reward => 0.472\n",
      " Ep. 151  *  Avg Reward => 0.473\n",
      " Ep. 152  *  Avg Reward => 0.474\n",
      " Ep. 153  *  Avg Reward => 0.473\n",
      " Ep. 154  *  Avg Reward => 0.472\n",
      " Ep. 155  *  Avg Reward => 0.469\n",
      " Ep. 156  *  Avg Reward => 0.469\n",
      " Ep. 157  *  Avg Reward => 0.466\n",
      " Ep. 158  *  Avg Reward => 0.465\n",
      " Ep. 159  *  Avg Reward => 0.463\n",
      " Ep. 160  *  Avg Reward => 0.461\n",
      " Ep. 161  *  Avg Reward => 0.460\n",
      " Ep. 162  *  Avg Reward => 0.460\n",
      " Ep. 163  *  Avg Reward => 0.459\n",
      " Ep. 164  *  Avg Reward => 0.459\n",
      " Ep. 165  *  Avg Reward => 0.459\n",
      " Ep. 166  *  Avg Reward => 0.461\n",
      " Ep. 167  *  Avg Reward => 0.461\n",
      " Ep. 168  *  Avg Reward => 0.461\n",
      " Ep. 169  *  Avg Reward => 0.461\n",
      " Ep. 170  *  Avg Reward => 0.460\n",
      " Ep. 171  *  Avg Reward => 0.460\n",
      " Ep. 172  *  Avg Reward => 0.460\n",
      " Ep. 173  *  Avg Reward => 0.458\n",
      " Ep. 174  *  Avg Reward => 0.458\n",
      " Ep. 175  *  Avg Reward => 0.459\n",
      " Ep. 176  *  Avg Reward => 0.459\n",
      " Ep. 177  *  Avg Reward => 0.459\n",
      " Ep. 178  *  Avg Reward => 0.458\n",
      " Ep. 179  *  Avg Reward => 0.458\n",
      " Ep. 180  *  Avg Reward => 0.459\n",
      " Ep. 181  *  Avg Reward => 0.459\n",
      " Ep. 182  *  Avg Reward => 0.459\n",
      " Ep. 183  *  Avg Reward => 0.458\n",
      " Ep. 184  *  Avg Reward => 0.457\n",
      " Ep. 185  *  Avg Reward => 0.457\n",
      " Ep. 186  *  Avg Reward => 0.456\n",
      " Ep. 187  *  Avg Reward => 0.455\n",
      " Ep. 188  *  Avg Reward => 0.453\n",
      " Ep. 189  *  Avg Reward => 0.452\n",
      " Ep. 190  *  Avg Reward => 0.451\n",
      " Ep. 191  *  Avg Reward => 0.451\n",
      " Ep. 192  *  Avg Reward => 0.451\n",
      " Ep. 193  *  Avg Reward => 0.451\n",
      " Ep. 194  *  Avg Reward => 0.450\n",
      " Ep. 195  *  Avg Reward => 0.449\n",
      " Ep. 196  *  Avg Reward => 0.449\n",
      " Ep. 197  *  Avg Reward => 0.449\n",
      " Ep. 198  *  Avg Reward => 0.449\n",
      " Ep. 199  *  Avg Reward => 0.450\n",
      " Ep. 200  *  Avg Reward => 0.451\n",
      " Ep. 201  *  Avg Reward => 0.453\n",
      " Ep. 202  *  Avg Reward => 0.454\n",
      " Ep. 203  *  Avg Reward => 0.456\n",
      " Ep. 204  *  Avg Reward => 0.456\n",
      " Ep. 205  *  Avg Reward => 0.456\n",
      " Ep. 206  *  Avg Reward => 0.455\n",
      " Ep. 207  *  Avg Reward => 0.456\n",
      " Ep. 208  *  Avg Reward => 0.457\n",
      " Ep. 209  *  Avg Reward => 0.456\n",
      " Ep. 210  *  Avg Reward => 0.456\n",
      " Ep. 211  *  Avg Reward => 0.456\n",
      " Ep. 212  *  Avg Reward => 0.454\n",
      " Ep. 213  *  Avg Reward => 0.453\n",
      " Ep. 214  *  Avg Reward => 0.452\n",
      " Ep. 215  *  Avg Reward => 0.453\n",
      " Ep. 216  *  Avg Reward => 0.454\n",
      " Ep. 217  *  Avg Reward => 0.452\n",
      " Ep. 218  *  Avg Reward => 0.452\n",
      " Ep. 219  *  Avg Reward => 0.453\n",
      " Ep. 220  *  Avg Reward => 0.452\n",
      " Ep. 221  *  Avg Reward => 0.451\n",
      " Ep. 222  *  Avg Reward => 0.452\n",
      " Ep. 223  *  Avg Reward => 0.453\n",
      " Ep. 224  *  Avg Reward => 0.454\n",
      " Ep. 225  *  Avg Reward => 0.453\n",
      " Ep. 226  *  Avg Reward => 0.451\n",
      " Ep. 227  *  Avg Reward => 0.450\n",
      " Ep. 228  *  Avg Reward => 0.451\n",
      " Ep. 229  *  Avg Reward => 0.452\n",
      " Ep. 230  *  Avg Reward => 0.452\n",
      " Ep. 231  *  Avg Reward => 0.452\n",
      " Ep. 232  *  Avg Reward => 0.451\n",
      " Ep. 233  *  Avg Reward => 0.450\n",
      " Ep. 234  *  Avg Reward => 0.450\n",
      " Ep. 235  *  Avg Reward => 0.451\n",
      " Ep. 236  *  Avg Reward => 0.451\n",
      " Ep. 237  *  Avg Reward => 0.450\n",
      " Ep. 238  *  Avg Reward => 0.451\n",
      " Ep. 239  *  Avg Reward => 0.450\n",
      " Ep. 240  *  Avg Reward => 0.450\n",
      " Ep. 241  *  Avg Reward => 0.451\n",
      " Ep. 242  *  Avg Reward => 0.451\n",
      " Ep. 243  *  Avg Reward => 0.450\n",
      " Ep. 244  *  Avg Reward => 0.449\n",
      " Ep. 245  *  Avg Reward => 0.448\n",
      " Ep. 246  *  Avg Reward => 0.448\n",
      " Ep. 247  *  Avg Reward => 0.449\n",
      " Ep. 248  *  Avg Reward => 0.450\n",
      " Ep. 249  *  Avg Reward => 0.450\n",
      " Ep. 250  *  Avg Reward => 0.450\n",
      " Ep. 251  *  Avg Reward => 0.450\n",
      " Ep. 252  *  Avg Reward => 0.451\n",
      " Ep. 253  *  Avg Reward => 0.452\n",
      " Ep. 254  *  Avg Reward => 0.452\n",
      " Ep. 255  *  Avg Reward => 0.453\n",
      " Ep. 256  *  Avg Reward => 0.452\n",
      " Ep. 257  *  Avg Reward => 0.458\n",
      " Ep. 258  *  Avg Reward => 0.457\n",
      " Ep. 259  *  Avg Reward => 0.456\n",
      " Ep. 260  *  Avg Reward => 0.456\n",
      " Ep. 261  *  Avg Reward => 0.456\n",
      " Ep. 262  *  Avg Reward => 0.456\n",
      " Ep. 263  *  Avg Reward => 0.457\n",
      " Ep. 264  *  Avg Reward => 0.457\n",
      " Ep. 265  *  Avg Reward => 0.458\n",
      " Ep. 266  *  Avg Reward => 0.459\n",
      " Ep. 267  *  Avg Reward => 0.459\n",
      " Ep. 268  *  Avg Reward => 0.459\n",
      " Ep. 269  *  Avg Reward => 0.458\n",
      " Ep. 270  *  Avg Reward => 0.457\n",
      " Ep. 271  *  Avg Reward => 0.457\n",
      " Ep. 272  *  Avg Reward => 0.457\n",
      " Ep. 273  *  Avg Reward => 0.457\n",
      " Ep. 274  *  Avg Reward => 0.458\n",
      " Ep. 275  *  Avg Reward => 0.459\n",
      " Ep. 276  *  Avg Reward => 0.460\n",
      " Ep. 277  *  Avg Reward => 0.461\n",
      " Ep. 278  *  Avg Reward => 0.461\n",
      " Ep. 279  *  Avg Reward => 0.461\n",
      " Ep. 280  *  Avg Reward => 0.461\n",
      " Ep. 281  *  Avg Reward => 0.460\n",
      " Ep. 282  *  Avg Reward => 0.460\n",
      " Ep. 283  *  Avg Reward => 0.461\n",
      " Ep. 284  *  Avg Reward => 0.462\n",
      " Ep. 285  *  Avg Reward => 0.463\n",
      " Ep. 286  *  Avg Reward => 0.463\n",
      " Ep. 287  *  Avg Reward => 0.463\n",
      " Ep. 288  *  Avg Reward => 0.462\n",
      " Ep. 289  *  Avg Reward => 0.462\n",
      " Ep. 290  *  Avg Reward => 0.461\n",
      " Ep. 291  *  Avg Reward => 0.460\n",
      " Ep. 292  *  Avg Reward => 0.460\n",
      " Ep. 293  *  Avg Reward => 0.460\n",
      " Ep. 294  *  Avg Reward => 0.460\n",
      " Ep. 295  *  Avg Reward => 0.460\n",
      " Ep. 296  *  Avg Reward => 0.460\n",
      " Ep. 297  *  Avg Reward => 0.460\n",
      " Ep. 298  *  Avg Reward => 0.460\n",
      " Ep. 299  *  Avg Reward => 0.460\n",
      " Ep. 300  *  Avg Reward => 0.461\n",
      " Ep. 301  *  Avg Reward => 0.462\n",
      " Ep. 302  *  Avg Reward => 0.463\n",
      " Ep. 303  *  Avg Reward => 0.463\n",
      " Ep. 304  *  Avg Reward => 0.462\n",
      " Ep. 305  *  Avg Reward => 0.462\n",
      " Ep. 306  *  Avg Reward => 0.462\n",
      " Ep. 307  *  Avg Reward => 0.461\n",
      " Ep. 308  *  Avg Reward => 0.462\n",
      " Ep. 309  *  Avg Reward => 0.463\n",
      " Ep. 310  *  Avg Reward => 0.464\n",
      " Ep. 311  *  Avg Reward => 0.464\n",
      " Ep. 312  *  Avg Reward => 0.464\n",
      " Ep. 313  *  Avg Reward => 0.464\n",
      " Ep. 314  *  Avg Reward => 0.464\n",
      " Ep. 315  *  Avg Reward => 0.463\n",
      " Ep. 316  *  Avg Reward => 0.462\n",
      " Ep. 317  *  Avg Reward => 0.461\n",
      " Ep. 318  *  Avg Reward => 0.462\n",
      " Ep. 319  *  Avg Reward => 0.463\n",
      " Ep. 320  *  Avg Reward => 0.464\n",
      " Ep. 321  *  Avg Reward => 0.464\n",
      " Ep. 322  *  Avg Reward => 0.465\n",
      " Ep. 323  *  Avg Reward => 0.466\n",
      " Ep. 324  *  Avg Reward => 0.466\n",
      " Ep. 325  *  Avg Reward => 0.465\n",
      " Ep. 326  *  Avg Reward => 0.465\n",
      " Ep. 327  *  Avg Reward => 0.465\n",
      " Ep. 328  *  Avg Reward => 0.465\n",
      " Ep. 329  *  Avg Reward => 0.465\n",
      " Ep. 330  *  Avg Reward => 0.464\n",
      " Ep. 331  *  Avg Reward => 0.465\n",
      " Ep. 332  *  Avg Reward => 0.464\n",
      " Ep. 333  *  Avg Reward => 0.464\n",
      " Ep. 334  *  Avg Reward => 0.463\n",
      " Ep. 335  *  Avg Reward => 0.462\n",
      " Ep. 336  *  Avg Reward => 0.461\n",
      " Ep. 337  *  Avg Reward => 0.460\n",
      " Ep. 338  *  Avg Reward => 0.459\n",
      " Ep. 339  *  Avg Reward => 0.459\n",
      " Ep. 340  *  Avg Reward => 0.459\n",
      " Ep. 341  *  Avg Reward => 0.459\n",
      " Ep. 342  *  Avg Reward => 0.460\n",
      " Ep. 343  *  Avg Reward => 0.459\n",
      " Ep. 344  *  Avg Reward => 0.459\n",
      " Ep. 345  *  Avg Reward => 0.460\n",
      " Ep. 346  *  Avg Reward => 0.460\n",
      " Ep. 347  *  Avg Reward => 0.461\n",
      " Ep. 348  *  Avg Reward => 0.461\n",
      " Ep. 349  *  Avg Reward => 0.461\n",
      " Ep. 350  *  Avg Reward => 0.461\n",
      " Ep. 351  *  Avg Reward => 0.460\n",
      " Ep. 352  *  Avg Reward => 0.459\n",
      " Ep. 353  *  Avg Reward => 0.458\n",
      " Ep. 354  *  Avg Reward => 0.457\n",
      " Ep. 355  *  Avg Reward => 0.457\n",
      " Ep. 356  *  Avg Reward => 0.456\n",
      " Ep. 357  *  Avg Reward => 0.457\n",
      " Ep. 358  *  Avg Reward => 0.456\n",
      " Ep. 359  *  Avg Reward => 0.456\n",
      " Ep. 360  *  Avg Reward => 0.456\n",
      " Ep. 361  *  Avg Reward => 0.456\n",
      " Ep. 362  *  Avg Reward => 0.456\n",
      " Ep. 363  *  Avg Reward => 0.455\n",
      " Ep. 364  *  Avg Reward => 0.455\n",
      " Ep. 365  *  Avg Reward => 0.455\n",
      " Ep. 366  *  Avg Reward => 0.455\n",
      " Ep. 367  *  Avg Reward => 0.455\n",
      " Ep. 368  *  Avg Reward => 0.456\n",
      " Ep. 369  *  Avg Reward => 0.455\n",
      " Ep. 370  *  Avg Reward => 0.455\n",
      " Ep. 371  *  Avg Reward => 0.455\n",
      " Ep. 372  *  Avg Reward => 0.456\n",
      " Ep. 373  *  Avg Reward => 0.457\n",
      " Ep. 374  *  Avg Reward => 0.457\n",
      " Ep. 375  *  Avg Reward => 0.457\n",
      " Ep. 376  *  Avg Reward => 0.456\n",
      " Ep. 377  *  Avg Reward => 0.455\n",
      " Ep. 378  *  Avg Reward => 0.454\n",
      " Ep. 379  *  Avg Reward => 0.453\n",
      " Ep. 380  *  Avg Reward => 0.453\n",
      " Ep. 381  *  Avg Reward => 0.452\n",
      " Ep. 382  *  Avg Reward => 0.451\n",
      " Ep. 383  *  Avg Reward => 0.450\n",
      " Ep. 384  *  Avg Reward => 0.450\n",
      " Ep. 385  *  Avg Reward => 0.450\n",
      " Ep. 386  *  Avg Reward => 0.449\n",
      " Ep. 387  *  Avg Reward => 0.449\n",
      " Ep. 388  *  Avg Reward => 0.449\n",
      " Ep. 389  *  Avg Reward => 0.449\n",
      " Ep. 390  *  Avg Reward => 0.450\n",
      " Ep. 391  *  Avg Reward => 0.450\n",
      " Ep. 392  *  Avg Reward => 0.450\n",
      " Ep. 393  *  Avg Reward => 0.449\n",
      " Ep. 394  *  Avg Reward => 0.449\n",
      " Ep. 395  *  Avg Reward => 0.448\n",
      " Ep. 396  *  Avg Reward => 0.447\n",
      " Ep. 397  *  Avg Reward => 0.446\n",
      " Ep. 398  *  Avg Reward => 0.446\n",
      " Ep. 399  *  Avg Reward => 0.446\n",
      " Ep. 400  *  Avg Reward => 0.446\n",
      " Ep. 401  *  Avg Reward => 0.447\n",
      " Ep. 402  *  Avg Reward => 0.447\n",
      " Ep. 403  *  Avg Reward => 0.447\n",
      " Ep. 404  *  Avg Reward => 0.447\n",
      " Ep. 405  *  Avg Reward => 0.448\n",
      " Ep. 406  *  Avg Reward => 0.448\n",
      " Ep. 407  *  Avg Reward => 0.449\n",
      " Ep. 408  *  Avg Reward => 0.449\n",
      " Ep. 409  *  Avg Reward => 0.450\n",
      " Ep. 410  *  Avg Reward => 0.451\n",
      " Ep. 411  *  Avg Reward => 0.451\n",
      " Ep. 412  *  Avg Reward => 0.451\n",
      " Ep. 413  *  Avg Reward => 0.450\n",
      " Ep. 414  *  Avg Reward => 0.450\n",
      " Ep. 415  *  Avg Reward => 0.451\n",
      " Ep. 416  *  Avg Reward => 0.451\n",
      " Ep. 417  *  Avg Reward => 0.451\n",
      " Ep. 418  *  Avg Reward => 0.450\n",
      " Ep. 419  *  Avg Reward => 0.450\n",
      " Ep. 420  *  Avg Reward => 0.450\n",
      " Ep. 421  *  Avg Reward => 0.451\n",
      " Ep. 422  *  Avg Reward => 0.450\n",
      " Ep. 423  *  Avg Reward => 0.449\n",
      " Ep. 424  *  Avg Reward => 0.449\n",
      " Ep. 425  *  Avg Reward => 0.448\n",
      " Ep. 426  *  Avg Reward => 0.449\n",
      " Ep. 427  *  Avg Reward => 0.449\n",
      " Ep. 428  *  Avg Reward => 0.450\n",
      " Ep. 429  *  Avg Reward => 0.451\n",
      " Ep. 430  *  Avg Reward => 0.451\n",
      " Ep. 431  *  Avg Reward => 0.452\n",
      " Ep. 432  *  Avg Reward => 0.453\n",
      " Ep. 433  *  Avg Reward => 0.454\n",
      " Ep. 434  *  Avg Reward => 0.453\n",
      " Ep. 435  *  Avg Reward => 0.453\n",
      " Ep. 436  *  Avg Reward => 0.453\n",
      " Ep. 437  *  Avg Reward => 0.453\n",
      " Ep. 438  *  Avg Reward => 0.452\n",
      " Ep. 439  *  Avg Reward => 0.453\n",
      " Ep. 440  *  Avg Reward => 0.452\n",
      " Ep. 441  *  Avg Reward => 0.452\n",
      " Ep. 442  *  Avg Reward => 0.453\n",
      " Ep. 443  *  Avg Reward => 0.453\n",
      " Ep. 444  *  Avg Reward => 0.453\n",
      " Ep. 445  *  Avg Reward => 0.452\n",
      " Ep. 446  *  Avg Reward => 0.453\n",
      " Ep. 447  *  Avg Reward => 0.453\n",
      " Ep. 448  *  Avg Reward => 0.454\n",
      " Ep. 449  *  Avg Reward => 0.454\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------convergence------------------------------------\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "total_episodes = 450\n",
    "max_step_per_episode = 450\n",
    "max_running_times = 2\n",
    "learning_rates = [0.001] # 0.003,0.01\n",
    "\n",
    "# def fus_txt(ep,action):\n",
    "#     fus_li = []\n",
    "#     ac_fus = np.zeros(total_episodes)\n",
    "#     if action[0] == 'None':\n",
    "#         pass\n",
    "#     else: \n",
    "#         fus_li.append(action[0])\n",
    "    \n",
    "#     ac_fus[ep] = np.mean(fus_li)\n",
    "#     return ac_fus\n",
    "\n",
    "\n",
    "def train(learning_rates=learning_rates, iterations=1):\n",
    "    \"Convergence performance with different learning rates.\"\n",
    "    # Create an environment\n",
    "    env = UAV()\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        output = './convergence_learning_rate_' + str(learning_rate) + '.txt'\n",
    "        output_fus = './convergence_fus_' + str(learning_rate) + '.txt'\n",
    "        \n",
    "        # Run several times and get the average results\n",
    "        count = 1\n",
    "        iteration = 1\n",
    "        while iteration <= iterations:\n",
    "            tf.keras.backend.clear_session()\n",
    "            print(\"\\n====== Learning rate ====== :\", learning_rate)\n",
    "            print(\"------ Iteration: {}/{}\".format(iteration,iterations))\n",
    "            \n",
    "            # Employ a new agent\n",
    "            agent = DDPG(env, learning_rate=learning_rate)\n",
    "            \n",
    "            # Train the ddpg agent\n",
    "            ep_reward_list = []\n",
    "            fus_li = []\n",
    "            avg_reward = np.zeros(total_episodes)\n",
    "            ac_fus = np.zeros(total_episodes)\n",
    "            # ac_fus = np.zeros(total_episodes)\n",
    "            fault = 0\n",
    "            for ep in range(total_episodes):\n",
    "                state = env.reset()\n",
    "                episodic_reward = 0\n",
    "                for _ in range(max_step_per_episode):\n",
    "                    # print(f'in  train state: {state}')\n",
    "                    action = agent.policy(state)\n",
    "                    reward, state_next = env.step(state, action)\n",
    "\n",
    "                    agent.record((state, action, reward, state_next))\n",
    "                    agent.update_model()\n",
    "                    agent.update_target()\n",
    "                # print(f'in train reward {reward} \\n episodic_reward {episodic_reward} \\n max_step_per_episode {max_step_per_episode} \\n')\n",
    "                    episodic_reward += reward\n",
    "                    state = state_next\n",
    "                ep_reward_list.append(episodic_reward/max_step_per_episode) #\n",
    "                # print(f'in train mean reward {episodic_reward}') # / max_step_per_episode\n",
    "               \n",
    "                nor_ep_reward = (ep_reward_list-min(ep_reward_list)) / (max(ep_reward_list) - min(ep_reward_list))\n",
    "               \n",
    "                avg_reward[ep] = np.mean(nor_ep_reward)\n",
    "\n",
    "                if action[0] == 'None':\n",
    "                    pass\n",
    "                else: \n",
    "                    fus_li.append(action[0])\n",
    "\n",
    "                ac_fus[ep] = np.mean(fus_li)\n",
    "\n",
    "                # fault = fault + 1 if avg_reward[ep] > avg_reward[ep-1]-10 else 0\n",
    "                print(\" Ep. {}  *  Avg Reward => {:.3f}\".format(ep, avg_reward[ep]))\n",
    "                # if fault == 5:\n",
    "                    # print('=====>>>> Restart the training loop <<<<=====')\n",
    "                    # break\n",
    "                # else:\n",
    "                # if not(os.path.isfile(output)):\n",
    "                #     np.savetxt(output, avg_reward,  fmt='%.3f', delimiter=',')\n",
    "                # else:\n",
    "                #     R = np.loadtxt(output, delimiter=',').reshape((-1,total_episodes))\n",
    "                #     # R_r = R.reshape(-1,total_episodes)\n",
    "                #     temp = np.mean(R, axis=0)\n",
    "                #     if ((learning_rate==0.01) & (avg_reward[-1] > temp[-1])) or ((learning_rate!=0.01) & (avg_reward[-1] < temp[-1])):\n",
    "                # R = np.append(R,avg_reward.reshape((1,total_episodes)),axis=0)\n",
    "                np.savetxt(output, avg_reward,  fmt='%.3f', delimiter=',')\n",
    "                np.savetxt(output_fus, ac_fus, fmt='%.8f', delimiter=',')\n",
    "                    # else:\n",
    "                    #     if count < max_running_times:\n",
    "                    #         count += 1\n",
    "                    #         # print(\"Result is not satisfied ==> Run again.\")\n",
    "                    #         continue\n",
    "                    #     else:\n",
    "                    #         count = 1\n",
    "                iteration += 1\n",
    "\n",
    "def plot(learning_rates=learning_rates):\n",
    "    # Create a figure and its twin.\n",
    "    fig, ax = plt.subplots()\n",
    "    # axins = zoomed_inset_axes(ax, zoom=25, loc='upper right', bbox_to_anchor=([235,215]))\n",
    "    \n",
    "    ticks = np.append(np.arange(0,100,20),[99])\n",
    "    ticklabels = np.append([1],np.arange(20,100+1,20))\n",
    "    marks = np.concatenate((np.arange(0,100,step=10),[99])).tolist()\n",
    "    lines = cycle([\"o-\",\"s--\",\"d-.\",\"*:\"])\n",
    "    for i in range(len(learning_rates)):\n",
    "        line_style = next(lines)\n",
    "        output = './convergence_learning_rate_' + str(learning_rates[i]) + '.txt'\n",
    "\n",
    "        R = np.loadtxt(output, delimiter=',').reshape((-1,total_episodes))\n",
    "        R = np.mean(R, axis=0)\n",
    "        ax.plot(R, line_style, label='Learning rate = {}'.format(learning_rates[i]), markevery=marks)\n",
    "        # axins.plot(R, line_style)\n",
    "    \n",
    "    ax.set_ylim(100,450)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(ticklabels)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average reward')\n",
    "    \n",
    "    # axins.set_xlim(79.4, 80.6) # apply the x-limits\n",
    "    # axins.set_ylim(-2.21, -1.99)    # apply the y-limits\n",
    "    mark_inset(ax, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "    # axins.set_xticks([])\n",
    "    # axins.set_yticks([])\n",
    "    \n",
    "    plt.savefig('./convergence_learning_rate.pdf', bbox_inches='tight')\n",
    "\n",
    "import sys\n",
    "sys.argv=['']\n",
    "del sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    # Set the input argument\n",
    "    parser = argparse.ArgumentParser(description='Convergence analysis')\n",
    "    parser.add_argument(\"-lr\",\"--learning_rate\", type=float, nargs='+', default=learning_rates, \n",
    "                        help=\"Learning rate of the proposed algorithm\")\n",
    "    parser.add_argument(\"-it\",\"--iteration\", type=int, default=1, help=\"number of training iteration.\")\n",
    "    \n",
    "    # Get the input argument\n",
    "    args = parser.parse_args()\n",
    "    learning_rates = args.learning_rate\n",
    "    iterations = args.iteration\n",
    "    \n",
    "    # Use the argument in function\n",
    "    train(learning_rates=learning_rates, iterations=iterations)\n",
    "    # plot(learning_rates=learning_rates)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e083a8acbcc16e669376690a634c6c85b0761b71c2427f0aa5c6cf3137ac454"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
