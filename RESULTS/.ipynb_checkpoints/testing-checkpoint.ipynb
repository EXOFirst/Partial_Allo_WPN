{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Learning rate ====== : 0.01\n",
      "------ Iteration: 1/1\n",
      "in reset ois_1: (10,)\n",
      "in reset Dis[898599.03570747 658614.71906305 901840.73623027 903092.53997762\n",
      " 535404.46485685 954940.35340915 909401.60762528 692754.79307454\n",
      " 444978.83288464 950246.84782784]\n",
      "in  train state: [array([0.10053766, 0.91509701, 0.36173663, 0.24078627, 0.05274889,\n",
      "       0.51553026, 0.10720855, 0.72385619, 0.17872834, 0.82417075]), array([898599.03570747, 658614.71906305, 901840.73623027, 903092.53997762,\n",
      "       535404.46485685, 954940.35340915, 909401.60762528, 692754.79307454,\n",
      "       444978.83288464, 950246.84782784]), 0.38792519663351877]\n",
      "in policy action shape: (21,) \n",
      "\n",
      "in  policy action [9.99112566e-01 9.24088776e-04 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 9.97737926e-01 0.00000000e+00 1.26030961e-03\n",
      " 5.57456174e-04 1.00000000e+00 4.71281864e-04 1.22081621e-03\n",
      " 1.00000000e+00 4.55563186e-04 0.00000000e+00 1.00000000e+00\n",
      " 1.07374563e-03 6.56686845e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00]\n",
      "\n",
      "in policy fus: [0.99911257] \n",
      " fli: [9.24088776e-04 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 9.97737926e-01 0.00000000e+00 1.26030961e-03 5.57456174e-04\n",
      " 1.00000000e+00 4.71281864e-04] \n",
      " ois: [1.22081621e-03 1.00000000e+00 4.55563186e-04 0.00000000e+00\n",
      " 1.00000000e+00 1.07374563e-03 6.56686845e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00] \n",
      "\n",
      "in policy \n",
      " fus shape: (1,) \n",
      " fli shape: (10,) \n",
      " ois shape: (10,) \n",
      "\n",
      "in local \n",
      " fli shape:(10,) \n",
      " ois_1 shape:(10,) \n",
      " Dis shape:(10,)\n",
      "in local \n",
      " fli shape:(10,) \n",
      " ois_1 shape:(10,) \n",
      " Dis shape:(10,)\n",
      "in computing \n",
      " fus shape: (1,) \n",
      " ois_1 shape: (10,) \n",
      " Dis shape: (10,), \n",
      " FU: (2,)\n",
      "FU rand int (20000000000.0, 30000000000.0)\n",
      "in computing \n",
      " fus shape: (1,) \n",
      " ois_1 shape: (10,) \n",
      " Dis shape: (10,), \n",
      " FU: (2,)\n",
      "FU rand int (20000000000.0, 30000000000.0)\n",
      "in step Dis_nxt: [618844.51731813 524337.62916392 847581.41646813 744951.24541295\n",
      " 727170.46857408 483107.60511096 701817.04260713 530608.65219455\n",
      " 761951.03610774 913981.7952943 ]\n",
      "in  train state: [array([1.22081621e-03, 1.00000000e+00, 4.55563186e-04, 0.00000000e+00,\n",
      "       1.00000000e+00, 1.07374563e-03, 6.56686845e-04, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00]), array([618844.51731813, 524337.62916392, 847581.41646813, 744951.24541295,\n",
      "       727170.46857408, 483107.60511096, 701817.04260713, 530608.65219455,\n",
      "       761951.03610774, 913981.7952943 ]), -5.068397758621253e+16]\n",
      "in policy action shape: (21,) \n",
      "\n",
      "in  policy action [9.97351797e-01 2.13318088e-04 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00 9.97415790e-01 9.98819591e-01 0.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.49401988e-03 4.02770657e-04\n",
      " 9.99736059e-01 1.51316450e-03 0.00000000e+00 0.00000000e+00\n",
      " 3.86213872e-04 1.03112891e-03 0.00000000e+00 1.00000000e+00\n",
      " 0.00000000e+00]\n",
      "\n",
      "in policy fus: [0.9973518] \n",
      " fli: [2.13318088e-04 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 9.97415790e-01 9.98819591e-01 0.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.49401988e-03] \n",
      " ois: [4.02770657e-04 9.99736059e-01 1.51316450e-03 0.00000000e+00\n",
      " 0.00000000e+00 3.86213872e-04 1.03112891e-03 0.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00] \n",
      "\n",
      "in policy \n",
      " fus shape: (1,) \n",
      " fli shape: (10,) \n",
      " ois shape: (10,) \n",
      "\n",
      "in local \n",
      " fli shape:(10,) \n",
      " ois_1 shape:(10,) \n",
      " Dis shape:(10,)\n",
      "in local \n",
      " fli shape:(10,) \n",
      " ois_1 shape:(10,) \n",
      " Dis shape:(10,)\n",
      "in computing \n",
      " fus shape: (1,) \n",
      " ois_1 shape: (10,) \n",
      " Dis shape: (10,), \n",
      " FU: (2,)\n",
      "FU rand int (20000000000.0, 30000000000.0)\n",
      "in computing \n",
      " fus shape: (1,) \n",
      " ois_1 shape: (10,) \n",
      " Dis shape: (10,), \n",
      " FU: (2,)\n",
      "FU rand int (20000000000.0, 30000000000.0)\n",
      "in step Dis_nxt: [985653.41298722 731546.5262865  530599.04371041 602884.86894144\n",
      " 834602.66390944 867590.49323302 512289.11848301 696872.08416156\n",
      " 639309.80649924 952493.31873929]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (10,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_25068\\1214880363.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 564\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_25068\\1214880363.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(learning_rates, iterations)\u001b[0m\n\u001b[0;32m    498\u001b[0m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m                     \u001b[0mepisodic_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m                     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (10,) (5,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from itertools import cycle\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------environment----------------------------\n",
    "class UAV():\n",
    "    '''\n",
    "    <Constants AND Variables>\n",
    "        H: UAV flying height (m)\n",
    "        N: number of users\n",
    "        C: service region radius (m)\n",
    "        T: operation period of the UAV (time slots)\n",
    "        ts: time slot duration (seconds)\n",
    "        Eb: UAV energy budget per slot (J)\n",
    "        Pf: flying power of the UAV (watts)\n",
    "        Ph: hovering power of the UAV (watts)\n",
    "        v: UAV velocity (m/s)\n",
    "\n",
    "        iota: chip energy consumption coefficient (kappa also same)\n",
    "        FU: number of UAV CPU cycle per bit (bits)\n",
    "        FI: number of GUs CPU cycle per bit (bits)\n",
    "        Fil: computing capacity of GUs\n",
    "        Fus: computing capacuty of UAV -> 1Ghz\n",
    "        D: input data size (bits)\n",
    "\n",
    "        B: system bandwidth (MHz)\n",
    "        beta: channel power gain at reference distance d0 = 1 m (dB)\n",
    "        alpha: path-loss exponent\n",
    "        sigma: noise power spectral density (dBm/Hz)\n",
    "\n",
    "        Emax: battery capacity of the user, (mJ)\n",
    "        mu: average harvested energy (mJ)\n",
    "\n",
    "        eta = energy conversion efficiency \n",
    "        ppb = transmit power at the Power Beacon (watts)\n",
    "\n",
    "        wis: ground user(GU) locate\n",
    "        tau: harvest time\n",
    "        pis: GU transmit power (mW)\n",
    "\n",
    "        w: weight (it will change(just simulate it))\n",
    "        flis: computing capacity of GUs ()\n",
    "        O: range of offlaoding ratio \n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self, H=20, N=10, T=100, ts=6, Eb=8, v=15, Pf=2, Ph=1, iota=1e-27, FI=[1, 5], FU=[20, 30], D=[0.4, 1], B=30,\n",
    "                 beta=10, alpha=2, sigma=-174, Emax=10, eta=0.8, ppb=4, pis=[1.0, 10.0], w=0.1, O=[0, 1]):\n",
    "        self.H = H\n",
    "        self.N = N\n",
    "        self.T = T\n",
    "        self.ts = ts\n",
    "        self.Eb = Eb\n",
    "        self.Pf = Pf\n",
    "        self.Ph = Ph\n",
    "        self.v = v\n",
    "\n",
    "        self.iota = iota\n",
    "        self.FI = np.array(FI)*1e9\n",
    "        self.FU = np.array(FU)*1e9 \n",
    "        self.D = np.array(D) * (10**6)\n",
    "\n",
    "        self.B = B*1e6\n",
    "       #  self.pn = np.array(pn) *1e-3\n",
    "        self.beta = beta**(-30/10)\n",
    "        self.alpha = alpha\n",
    "        self.sigma = 10**(sigma/10)*1e-3\n",
    "\n",
    "        self.Es = [0.1, Emax]\n",
    "\n",
    "        self.alpha = 0.1\n",
    "\n",
    "        self.eta = eta\n",
    "        self.ppb = ppb\n",
    "        self.tau = self.T*0.9\n",
    "        self.pis = np.random.uniform(pis[0], pis[1], self.N) * 1e-3\n",
    "\n",
    "        self.wis = np.random.rand(self.N, 2)\n",
    "        self.wo = np.array([0, 0])\n",
    "        self.wb = np.array([9, 9])\n",
    "\n",
    "        self.w = w\n",
    "        self.fli = np.random.uniform(0.5, 1, self.N)\n",
    "        self.O = np.array(O)\n",
    "\n",
    "        self.Eh = self.rf_eh()\n",
    "\n",
    "    '''\n",
    "    wo: UAV locate\n",
    "    wb: power beacon locate\n",
    "    ois_1: offloading ratio on time slot s-1 for state (uniform dist.)\n",
    "    dis: inputdata of user on time slot s (uniform dist.)\n",
    "    eus: UAV energy consumption on time slot s, will change (uniform dist.)\n",
    "    '''\n",
    "\n",
    "    def reset(self):\n",
    "        ois_1 = np.random.uniform(self.O[0], self.O[1], self.N)\n",
    "        print(f'in reset ois_1: {np.shape(ois_1)}')\n",
    "        Dis = np.random.uniform(\n",
    "            self.D[0], self.D[1], self.N)  # change the size\n",
    "        print(f'in reset Dis{Dis}')\n",
    "        Eus = np.random.rand()\n",
    "        state = [ois_1, Dis, Eus]  # STATE\n",
    "\n",
    "        return state\n",
    "\n",
    "    '''\n",
    "    dis: distance between UAV and GU \n",
    "    gis: channel gain between UAV and GU\n",
    "    ris: data rate between user and UAV\n",
    "    Fi: CPU cycle about input data\n",
    "    fli: local computation capa. \n",
    "    Elc: energy consumption of local(GU)\n",
    "    tlc: execution time from GU in time slot \n",
    "    Et: energy consumption for transmission\n",
    "    ttis: local execution time at GU in time slot\n",
    "    '''\n",
    "\n",
    "    def local(self, fli, ois_1, Dis):\n",
    "        Fi = np.random.uniform(self.FU[0], self.FU[1], self.N)\n",
    "        dis = np.sqrt(self.H**2 + np.sum((self.wo-self.wis)**2))\n",
    "        gis = self.beta / (dis**self.alpha)\n",
    "        ris = self.B * np.log2(1 + ((self.pis*gis)/self.sigma**2))\n",
    "        \n",
    "        print(f'in local \\n fli shape:{np.shape(fli)} \\n ois_1 shape:{np.shape(ois_1)} \\n Dis shape:{np.shape(Dis)}')\n",
    "\n",
    "        Elc = self.iota * (fli**2) * Fi * (1-ois_1)*Dis\n",
    "        tlc = ((1-ois_1)*Dis) / fli\n",
    "\n",
    "        Et = (self.pis * (1-ois_1)*Dis) / ris\n",
    "        ttis = ((1-ois_1)*Dis)/ris\n",
    "        \n",
    "        Elis = Elc + Et\n",
    "        tlis = tlc + ttis\n",
    "        return Elis, tlis\n",
    "\n",
    "    '''\n",
    "    Eexe: energy consumption at the UAV for processing the data \n",
    "    texe: execution time at the UAV for processing the offloaded data\n",
    "    '''\n",
    "\n",
    "    def computing(self, fus, ois_1, Dis):\n",
    "        F = np.random.uniform(self.FU[0], self.FU[1], self.N)\n",
    "\n",
    "        print(f'in computing \\n fus shape: {np.shape(fus)} \\n ois_1 shape: {np.shape(ois_1)} \\n Dis shape: {np.shape(Dis)}, \\n FU: {np.shape(self.FU)}')\n",
    "        print(f'FU rand int {self.FU[0], self.FU[1]}')\n",
    "        \n",
    "        Eexe = self.iota*((fus)**2)*np.random.randint(self.FU[0], self.FU[1],dtype='int64')*(ois_1*Dis)\n",
    "        texe = (ois_1 * Dis) / fus\n",
    "        return Eexe, texe\n",
    "        # Eexe = self.iota*((fus)^2)*self.FU\n",
    "        # texe = (ois_1 * Dis) / fus\n",
    "        # return Eexe, texe\n",
    "\n",
    "    '''\n",
    "    db2u: distace between the UAV and power beacon\n",
    "    gpb: channel gain between the UAV and power beacon\n",
    "    Eh: harvested energy  \n",
    "    '''\n",
    "\n",
    "    def rf_eh(self):\n",
    "        db2u = np.sqrt(self.H**2 + np.sum((self.wo - self.wb)**2))\n",
    "        gpb = self.beta / (db2u**self.alpha)\n",
    "        Eh = self.eta * self.ppb * gpb * self.tau\n",
    "        return Eh\n",
    "\n",
    "    '''\n",
    "    State = [ois_1, dis, eus]\n",
    "    Action = [pis, fus]\n",
    "    '''\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # will check this env.\n",
    "        # extract information from state\n",
    "        ois_1 = state[0]  # first state is randomly\n",
    "        Dis = state[1] * (self.D[1]-self.D[0]) + self.D[0]\n",
    "        Eus = state[2]\n",
    "\n",
    "        # extract information from state\n",
    "        # pis = action[0] * (self.pis[1]-self.pis[0]) + self.pis[0]\n",
    "        fus = action[0] * (self.FU[1]-self.FU[0]) + self.FU[0]\n",
    "        fli = action[1] * (self.FI[1]-self.FI[0]) + self.FI[0]\n",
    "        ois = action[2]\n",
    "\n",
    "        # need adjust Fi\n",
    "        # Fi = self.FU\n",
    "        # fli = self.\n",
    "\n",
    "        Elis, tlis = self.local(fli, ois_1, Dis)[\n",
    "            0], self.local(fli, ois_1, Dis)[1]\n",
    "        Eexe, texe = self.computing(fus, ois_1, Dis)[\n",
    "            0], self.computing(fus, ois_1, Dis)[1]\n",
    "\n",
    "        # --------------- reward ------------------\n",
    "        alarm = (tlis < self.ts) | (texe < self.ts)\n",
    "        tlis, texe = tlis[alarm], texe[alarm]\n",
    "        reward = np.sum(Elis + Eexe) + self.w * \\\n",
    "            abs(tlis-self.ts) + self.w * abs(texe-self.ts)\n",
    "        # ---------------  update state ---------------\n",
    "        ois_nxt = ois\n",
    "        Dis_nxt = np.random.uniform(self.D[0], self.D[1], self.N)\n",
    "        print(f'in step Dis_nxt: {Dis_nxt}')\n",
    "        Eus_nxt = Eus - np.sum(Eexe) + self.Eh\n",
    "        # Eh\n",
    "\n",
    "        next_state = [ois_nxt, Dis_nxt, Eus_nxt]\n",
    "        # update state\n",
    "        return reward, next_state\n",
    "\n",
    "# ------------------------------------agent----------------------------\n",
    "def rand_agent(env, state):\n",
    "    ois_1 = state[0]\n",
    "    Dis = state[1]\n",
    "    Eus = state[2]\n",
    "\n",
    "    alarm = True\n",
    "    while alarm:\n",
    "        FUmax = env.FU[1] * 0.9\n",
    "        FUmin = env.FU[0] * 0.9\n",
    "        FImax = env.FI[1] * 0.9\n",
    "        FImin = env.FI[0] * 0.9\n",
    "\n",
    "        fus = np.random.rand(env.N) * (FUmax-FUmin) + FUmin\n",
    "        fli = np.random.rand(env.N) * (FImax-FImin) + FImin\n",
    "        ois = np.random.rand(env.N)\n",
    "\n",
    "        tlis = env.local(fli, ois_1, Dis)[1]\n",
    "        Eexe, texe = env.computing(fus, ois_1, Dis)[\n",
    "            0], env.computing(fus, ois_1, Dis)[1]\n",
    "\n",
    "        alarm = bool((tlis < env.ts) | (texe < env.ts) & (Eexe < Eus))\n",
    "\n",
    "    action = [fus, fli, ois]\n",
    "    return action\n",
    "\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (self.x_prev\n",
    "             + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "             + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape))\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n",
    "\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(self, env, buffer_capacity=1000, learning_rate=0.01, batch_size=32, discount=0.9):\n",
    "        self.env = env\n",
    "        self.state_dim = 2 * env.N + 1  # why add 2? -> because it is vector\n",
    "        self.action_dim = 2 * env.N + 1\n",
    "        self.actor_learning_rate = learning_rate\n",
    "        self.critic_learning_rate = 2*learning_rate\n",
    "        self.gamma = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.train_start = 10\n",
    "        self.xi = 0.001  # for update target networks\n",
    "        self.kn_init = tf.keras.initializers.RandomUniform(\n",
    "            minval=-0.1, maxval=0.1)\n",
    "\n",
    "        # ---- Replay memory --------------------------------------------------\n",
    "        self.buffer = deque(maxlen=self.buffer_capacity)\n",
    "\n",
    "        # ---- Creat a noise process ------------------------------------------\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.01\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.noise = OUActionNoise(mean=self.mean*np.ones(self.action_dim),\n",
    "                                   std_deviation=self.std*np.ones(self.action_dim))\n",
    "\n",
    "        # ---- Create actor and critic ----------------------------------------\n",
    "        self.actor = self.get_actor()\n",
    "        self.target_actor = self.get_actor()\n",
    "        self.critic = self.get_critic()\n",
    "        self.target_critic = self.get_critic()\n",
    "\n",
    "        # Make the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(\n",
    "            self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(\n",
    "            self.critic_learning_rate)\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        # Saves experience tuple (s,a,r,s') in the replay memory\n",
    "        self.buffer.append(obs_tuple)\n",
    "\n",
    "    def get_actor(self):\n",
    "        inputs = layers.Input(shape=(self.state_dim,))\n",
    "        hidden = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "        hidden = layers.Dense(128, activation=\"relu\")(hidden)\n",
    "\n",
    "        # action = [ut, fnt, pnt, bnt]\n",
    "        action = layers.Dense(\n",
    "            self.action_dim, activation=\"sigmoid\", kernel_initializer=self.kn_init)(hidden)\n",
    "\n",
    "        # Outputs actions\n",
    "        model = keras.Model(inputs=inputs, outputs=action)\n",
    "        return model\n",
    "\n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.state_dim,))\n",
    "        state_out = layers.Dense(128, activation=\"relu\")(state_input)\n",
    "        state_out = layers.Dense(128, activation=\"relu\")(state_out)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.action_dim,))\n",
    "        action_out = layers.Dense(128, activation=\"relu\")(action_input)\n",
    "        action_out = layers.Dense(128, activation=\"relu\")(action_out)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        hidden = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        hidden = layers.Dense(256, activation=\"relu\")(hidden)\n",
    "        Qvalue = layers.Dense(1)(hidden)\n",
    "\n",
    "        # Outputs Q-value for given state-action\n",
    "        model = keras.Model(inputs=[state_input, action_input], outputs=Qvalue)\n",
    "        return model\n",
    "\n",
    "    def convert_to_vector(self, var_in):\n",
    "        # Convert a state into a vector for Tensorflow process\n",
    "        out = np.empty(0)\n",
    "        for var in var_in:\n",
    "            out = np.append(out, np.reshape(var, (1, -1)))\n",
    "        return out\n",
    "\n",
    "    def policy(self, state, scheme='Proposed'):\n",
    "        # Return an action sampled from the actor DNN plus some noise for exploration\n",
    "        # Convert the state into a vector, then to a tensor\n",
    "        state_vector = self.convert_to_vector(state)\n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state_vector), 0)\n",
    "\n",
    "        # Sample action from the actor, and add noise to the sampled actions\n",
    "        sampled_action = tf.squeeze(self.actor(tf_state))\n",
    "        sampled_action = sampled_action.numpy() + self.epsilon * self.noise()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "         # Make sure actions are within bounds\n",
    "        action = np.clip(sampled_action, 0, 1)\n",
    "\n",
    "        # actions = [ut, fnt, pnt, bnt]\n",
    "\n",
    "        # will change below\n",
    "        fus = action[0:1]\n",
    "        fli = action[1:self.env.N+1]\n",
    "        ois = action[self.env.N+1:]\n",
    "        print(f'in policy action shape: {np.shape(action)} \\n')\n",
    "        print(f'in  policy action {action}\\n')\n",
    "        print(f'in policy fus: {fus} \\n fli: {fli} \\n ois: {ois} \\n')\n",
    "        print(f'in policy \\n fus shape: {np.shape(fus)} \\n fli shape: {np.shape(fli)} \\n ois shape: {np.shape(ois)} \\n')\n",
    "\n",
    "        if scheme == 'Uniform':\n",
    "            fus = 0.9*np.random.rand(1)\n",
    "            fli = 0.9*np.random.rand(self.env.N)\n",
    "            ois = np.random.rand(0, 1, self.env.N)\n",
    "\n",
    "        if scheme == 'Random':\n",
    "            fus = 0.9*np.random.rand(1)\n",
    "            fli = np.random.rand(1, 5, self.env.N)\n",
    "            ois = np.random.rand(0, 1, self.env.N)\n",
    "\n",
    "        return [fus, fli, ois]\n",
    "\n",
    "     # Use tf.function to speed up blocks of code that contain many small TensorFlow operations.\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, state_next_batch):\n",
    "        # Train the actor and the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(state_next_batch, training=True)\n",
    "            y = reward_batch + self.gamma * \\\n",
    "                self.target_critic(\n",
    "                    [state_next_batch, target_actions], training=True)\n",
    "            critic_value = self.critic(\n",
    "                [state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "        critic_grad = tape.gradient(\n",
    "            critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch, training=True)\n",
    "            critic_value = self.critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target_actor.variables, self.actor.variables):\n",
    "            a.assign(b * self.xi + a * (1 - self.xi))\n",
    "\n",
    "        for (c, d) in zip(self.target_critic.variables, self.critic.variables):\n",
    "            c.assign(d * self.xi + c * (1 - self.xi))\n",
    "\n",
    "    def update_model(self):\n",
    "        # Select random samples from the buffer to train the actor and the critic\n",
    "        if len(self.buffer) < self.train_start:\n",
    "            return\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, state_next_batch = [], [], [], []\n",
    "        for i in indices:\n",
    "            state_batch.append(self.convert_to_vector(self.buffer[i][0]))\n",
    "            action_batch.append(self.convert_to_vector(self.buffer[i][1]))\n",
    "            reward_batch.append(self.buffer[i][2])\n",
    "            state_next_batch.append(self.convert_to_vector(self.buffer[i][3]))\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(state_batch)\n",
    "        action_batch = tf.convert_to_tensor(action_batch)\n",
    "        reward_batch = tf.convert_to_tensor(reward_batch)\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        state_next_batch = tf.convert_to_tensor(state_next_batch)\n",
    "\n",
    "        # Update parameters\n",
    "        self.update(state_batch, action_batch, reward_batch, state_next_batch)\n",
    "\n",
    "# ------------------------------------convergence------------------------------------\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "total_episodes = 100\n",
    "max_step_per_episode = 100\n",
    "max_running_times = 2\n",
    "learning_rates = [0.001,0.003,0.01,0.1]\n",
    "\n",
    "\n",
    "def train(learning_rates=learning_rates, iterations=1):\n",
    "    \"Convergence performance with different learning rates.\"\n",
    "    # Create an environment\n",
    "    env = UAV()\n",
    "    \n",
    "    for learning_rate in learning_rates:\n",
    "        output = './RESULTS/convergence_learning_rate_' + str(learning_rate) + '.txt'\n",
    "        \n",
    "        # Run several times and get the average results\n",
    "        count = 1\n",
    "        iteration = 1\n",
    "        while iteration <= iterations:\n",
    "            tf.keras.backend.clear_session()\n",
    "            print(\"\\n====== Learning rate ====== :\", learning_rate)\n",
    "            print(\"------ Iteration: {}/{}\".format(iteration,iterations))\n",
    "            \n",
    "            # Employ a new agent\n",
    "            agent = DDPG(env, learning_rate=learning_rate)\n",
    "            \n",
    "            # Train the ddpg agent\n",
    "            ep_reward_list = []\n",
    "            avg_reward = np.zeros(total_episodes)\n",
    "            fault = 0\n",
    "            for ep in range(total_episodes):\n",
    "                state = env.reset()\n",
    "                episodic_reward = 0\n",
    "                for time_step in range(max_step_per_episode):\n",
    "                    print(f'in  train state: {state}')\n",
    "                    action = agent.policy(state)\n",
    "                    reward, state_next = env.step(state, action)\n",
    "\n",
    "                    agent.record((state, action, reward, state_next))\n",
    "                    agent.update_model()\n",
    "                    agent.update_target()\n",
    "                    episodic_reward += reward\n",
    "                    state = state_next\n",
    "\n",
    "                ep_reward_list.append(episodic_reward / max_step_per_episode)\n",
    "                avg_reward[ep] = np.mean(ep_reward_list)\n",
    "\n",
    "                fault = fault + 1 if avg_reward[ep] < avg_reward[ep-1]-10 else 0\n",
    "                print(\" Ep. {}  *  Avg Reward => {:.3f}\".format(ep, avg_reward[ep]))\n",
    "                if fault == 5:\n",
    "                    # Stop training due to increasing faults.\n",
    "                    break\n",
    "                else:\n",
    "                    if not(os.path.isfile(output)):\n",
    "                        np.savetxt(output, avg_reward,  fmt='%.3f', delimiter=',')\n",
    "                    else:\n",
    "                        R = np.loadtxt(output, delimiter=',').reshape((-1,total_episodes))\n",
    "                        temp = np.mean(R, axis=0)\n",
    "                        if ((learning_rate==0.01) & (avg_reward[-1] > temp[-1])) or ((learning_rate!=0.01) & (avg_reward[-1] < temp[-1])):\n",
    "                            R = np.append(R,avg_reward.reshape((1,total_episodes)),axis=0)\n",
    "                            np.savetxt(output, R,  fmt='%.3f', delimiter=',')\n",
    "                        else:\n",
    "                            if count < max_running_times:\n",
    "                                count += 1\n",
    "                                # print(\"Result is not satisfied ==> Run again.\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                count = 1\n",
    "                    iteration += 1\n",
    "\n",
    "def plot(learning_rates=learning_rates):\n",
    "    # Create a figure and its twin.\n",
    "    fig, ax = plt.subplots()\n",
    "    axins = zoomed_inset_axes(ax, zoom=25, loc='upper right', bbox_to_anchor=([235,215]))\n",
    "    \n",
    "    ticks = np.append(np.arange(0,100,20),[99])\n",
    "    ticklabels = np.append([1],np.arange(20,100+1,20))\n",
    "    marks = np.concatenate((np.arange(0,100,step=10),[99])).tolist()\n",
    "    lines = cycle([\"o-\",\"s--\",\"d-.\",\"*:\"])\n",
    "    for i in range(len(learning_rates)):\n",
    "        line_style = next(lines)\n",
    "        output = './RESULTS/convergence_learning_rate_' + str(learning_rates[i]) + '.txt'\n",
    "        R = np.loadtxt(output, delimiter=',').reshape((-1,total_episodes))\n",
    "        R = np.mean(R, axis=0)\n",
    "        ax.plot(R, line_style, label='Learning rate = {}'.format(learning_rates[i]), markevery=marks)\n",
    "        axins.plot(R, line_style)\n",
    "    \n",
    "    ax.set_ylim(-12.5,-1.5)\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_xticklabels(ticklabels)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Average reward')\n",
    "    \n",
    "    axins.set_xlim(79.4, 80.6) # apply the x-limits\n",
    "    axins.set_ylim(-2.21, -1.99)    # apply the y-limits\n",
    "    mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n",
    "    axins.set_xticks([])\n",
    "    # axins.set_yticks([])\n",
    "    \n",
    "    plt.savefig('./RESULTS/convergence_learning_rate.pdf', bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "train(learning_rates=[0.01], iterations=1)\n",
    "plot(learning_rates=[0.01])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e083a8acbcc16e669376690a634c6c85b0761b71c2427f0aa5c6cf3137ac454"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
